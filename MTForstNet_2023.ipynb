{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6d874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN#\n",
    "def MAIN_RF_MT(data_path,task_list,seed,problem_mode,main_perform,save_file_name):\n",
    "    read_preprocess(data_path,task_list,seed)\n",
    "\n",
    "    q=0\n",
    "    number = list(range(0,500))    \n",
    "    result_df = locals\n",
    "    result_df=pd.DataFrame(index=number)   \n",
    "    \n",
    "    regression_col_list=['Layer','Task' , \n",
    "              'Train_MSE','Train_RMSE','Train_MAE','Train_R2','Train_Pearsonr','Train_Median_AE',            \n",
    "            'Validation_MSE','Validation_RMSE','Validation_MAE','Validation_R2','Validation_Pearsonr','Validation_Median_AE',            \n",
    "            'Test_MSE','Test_RMSE','Test_MAE','Test_R2','Test_Pearsonr','Test_Median_AE']            \n",
    "\n",
    "    classification_col_list=['Layer',  'Task',\n",
    "            'Train AUC', 'Train ACC','Train Balance ACC','Train F1','Train Precision', 'Train Sensitivity','Train Specificity',\n",
    "            'Validation AUC','Validation ACC', 'Validation Balance ACC','Validation F1','Validation Precision','Validation Sensitivity','Validation Specificity',\n",
    "              'Test_AUC', 'Test ACC', 'Test Balance ACC','Test F1',\t 'Test Precision','Test Sensitivity','Test Specificity']\n",
    "    \n",
    "    if problem_mode=='classification':\n",
    "        col_list=classification_col_list\n",
    "    else:\n",
    "        col_list=regression_col_list\n",
    "\n",
    "    decision_df=pd.DataFrame()\n",
    "    i=0\n",
    "    for i in tqdm(range(0,10)):        \n",
    "        if decision_df.empty :\n",
    "            print('----training----'+str(i))  \n",
    "            for label in tqdm(labels):\n",
    "                train_perform(label,i,problem_mode,seed)\n",
    "                for r in range(len(col_list)):\n",
    "                    result_df.at[q,col_list[r]]=result_list[r]\n",
    "                result_df=result_df.round(3)   \n",
    "                q=q+1\n",
    "                \n",
    "            ####修改評估條件######\n",
    "            result_df['Validation '+str(main_perform)]=result_df['Validation '+str(main_perform)].abs()\n",
    "            validation_df_for_mean=result_df.loc[(result_df['Layer']==i)]                   \n",
    "            validation_mean=validation_df_for_mean['Validation '+str(main_perform)].mean(axis=0)\n",
    "            result_df.loc[(result_df['Layer']==i),'Mean '+str(main_perform)]=validation_mean            \n",
    "            result_df['Mean '+str(main_perform)]=result_df['Mean '+str(main_perform)].round(3)\n",
    "                \n",
    "            if main_perform =='AUC' or 'ACC' or 'Balance ACC' or 'F1' or 'Precision' or'Recall'or'Specificity'or'R2'or'Pearsonr':\n",
    "                if (i== 0) or (list(set(result_df.iloc[list(result_df.Layer== i-1) ,-1]))<list(set(result_df.iloc[list(result_df.Layer== i) ,-1]))):        \n",
    "                    evaluation_NF(labels,problem_mode,i)\n",
    "                else:\n",
    "                    print('開始存出Excel')\n",
    "                    result_df=result_df.round(3)\n",
    "                    result_df=result_df.dropna()\n",
    "\n",
    "                    result_df.to_csv('MTRFNET_'+str(save_file_name)+'_result.csv')\n",
    "                    decision_df=result_df\n",
    "                    #count average performance#\n",
    "                    file=result_df\n",
    "                    raw2mean(file,problem_mode,main_perform,save_file_name)                    \n",
    "\n",
    "            else:\n",
    "                if (i== 0) or (list(set(result_df.iloc[list(result_df.Layer== i-1) ,-1]))>list(set(result_df.iloc[list(result_df.Layer== i) ,-1]))):        \n",
    "                    evaluation_NF(labels,problem_mode,i)\n",
    "\n",
    "                else:\n",
    "                    print('開始存出Excel')\n",
    "                    result_df=result_df.round(3)\n",
    "                    result_df=result_df.dropna()\n",
    "\n",
    "                    result_df.to_csv('MTRFNET_'+str(save_file_name)+'_result.csv')\n",
    "                    decision_df=result_df\n",
    "                    #count average performance#\n",
    "                    file=result_df\n",
    "                    raw2mean(file,problem_mode,main_perform,save_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f45b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1#\n",
    "def read_preprocess(data_path,task_list,seed):\n",
    "    print('-----preprocessing-----')  \n",
    "    data=pd.read_csv(data_path)   \n",
    "    if 'Unnamed: 0' in data:\n",
    "        data=data.drop(columns=['Unnamed: 0'])\n",
    "    if 'SMILES' in data:\n",
    "        data=data.drop(columns=['SMILES'])\n",
    "    if 'smiles' in data:\n",
    "        data=data.drop(columns=['smiles'])    \n",
    "    if 'mol_id' in data:\n",
    "        data=data.drop(columns=['mol_id'])    \n",
    "\n",
    "###task_list###                   \n",
    "    global labels\n",
    "    labels=task_list\n",
    "\n",
    "    for i in data.index:\n",
    "        for q in labels:\n",
    "            try:\n",
    "                if data.loc[i,q]== 'ACT':\n",
    "                    data.loc[i,q]=int(1)\n",
    "                if data.loc[i,q]== 'INACT':           \n",
    "                    data.loc[i,q]=int(0)             \n",
    "            except:\n",
    "                pass   \n",
    "    for label in labels:\n",
    "        labels_to_drop=[l for l in labels if l !=label]\n",
    "        locals()['Task_'+str(label)]  = data.drop(labels_to_drop,axis=1)  \n",
    "        locals()['Task_'+str(label)]= locals()['Task_'+str(label)].dropna(subset=[label])\n",
    "    for label in labels: \n",
    "        globals()['Task_'+str(label)+'_train_0'],globals()['Task_'+str(label)+'_test_0'] = train_test_split(locals()['Task_'+str(label)], test_size=0.2,random_state=seed ,stratify=locals()['Task_'+str(label)][label])\n",
    "        globals()['Task_'+str(label)+'_train_0'],globals()['Task_'+str(label)+'_valid_0'] = train_test_split(globals()['Task_'+str(label)+'_train_0'], test_size=0.125,random_state=seed ,stratify=globals()['Task_'+str(label)+'_train_0'][label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91928c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2#\n",
    "def train_perform(label,i,problem_mode,seed):    \n",
    "    \n",
    "    y_train=globals()['Task_'+str(label)+'_train_'+str(i)][label].astype('float')\n",
    "    x_train=globals()['Task_'+str(label)+'_train_'+str(i)].drop(columns=label)    \n",
    "    y_valid=globals()['Task_'+str(label)+'_valid_'+str(i)][label].astype('float')\n",
    "    x_valid=globals()['Task_'+str(label)+'_valid_'+str(i)].drop(columns=label)   \n",
    "    y_test=globals()['Task_'+str(label)+'_test_'+str(i)][label].astype('float')\n",
    "    x_test=globals()['Task_'+str(label)+'_test_'+str(i)].drop(columns=label)    \n",
    "    \n",
    "    ##model_parameter##\n",
    "    if problem_mode=='classification':\n",
    "        model_parameter=RandomForestClassifier(n_estimators=500,max_features='log2',random_state=seed,n_jobs=-2)\n",
    "    else:\n",
    "        model_parameter=RandomForestRegressor(n_estimators=500,max_features='log2',random_state=seed,n_jobs=-2)\n",
    "           \n",
    "    x_train=x_train.astype(float)\n",
    "    y_train=y_train.astype(float)    \n",
    "\n",
    "    ##model##\n",
    "    globals()['predictor_'+str(label)+'_'+str(i)]=model_parameter.fit(x_train, y_train)\n",
    "    ###儲存模型###\n",
    "    #joblib.dump(globals()['predictor_'+str(label)+'_'+str(i)], './Zebrafish_Models_withALLMorphology_ECFP6_202310_prob/predictor_'+str(label)+'_'+str(i))\n",
    "\n",
    "    ###    \n",
    "    locals()['y_train_true']=(y_train)\n",
    "    locals()['y_valid_true']=(y_valid)\n",
    "    locals()['y_test_true']=(y_test)\n",
    "\n",
    "    locals()['y_train_pred']=globals()['predictor_'+str(label)+'_'+str(i)].predict(x_train)\n",
    "    locals()['y_valid_pred']=globals()['predictor_'+str(label)+'_'+str(i)].predict(x_valid)\n",
    "    locals()['y_test_pred']=globals()['predictor_'+str(label)+'_'+str(i)].predict(x_test)\n",
    "\n",
    "    locals()['y_prob_train']=globals()['predictor_'+str(label)+'_'+str(i)].predict_proba(x_train)[:, 1]\n",
    "    locals()['y_prob_test']=globals()['predictor_'+str(label)+'_'+str(i)].predict_proba(x_test)[:, 1]\n",
    "    locals()['y_prob_valid']=globals()['predictor_'+str(label)+'_'+str(i)].predict_proba(x_valid)[:, 1]\n",
    "\n",
    "    ##train_performances##\n",
    "    global result_list\n",
    "    result_list=[]\n",
    "    result_list.append(int(i))\n",
    "    result_list.append(label)\n",
    "    \n",
    "    if problem_mode == 'classification':\n",
    "        \n",
    "        p_list=['train','valid','test']\n",
    "        for p in p_list:\n",
    "            \n",
    "            p_true=locals()['y_'+str(p)+'_true']\n",
    "            p_prob=locals()['y_prob_'+str(p)]\n",
    "            p_pred=locals()['y_'+str(p)+'_pred']\n",
    "            \n",
    "            locals()[str(p)+'_AUC'] =metrics.roc_auc_score(p_true,p_prob).round(3) \n",
    "            locals()[str(p)+'_ACC'] =metrics.accuracy_score(p_true, p_pred).round(3)   \n",
    "            locals()[str(p)+'_BACC']=metrics.balanced_accuracy_score(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_F1'] =metrics.f1_score(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_PR'] =metrics.precision_score(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_R']=metrics.recall_score(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_S'] =metrics.recall_score(p_true, p_pred, pos_label=0).round(3)\n",
    "\n",
    "            result_list.append(locals()[str(p)+'_AUC'])\n",
    "            result_list.append(locals()[str(p)+'_ACC'])\n",
    "            result_list.append(locals()[str(p)+'_BACC'])    \n",
    "            result_list.append(locals()[str(p)+'_F1'])\n",
    "            result_list.append(locals()[str(p)+'_PR'])\n",
    "            result_list.append(locals()[str(p)+'_R'])\n",
    "            result_list.append(locals()[str(p)+'_S'])        \n",
    "    else:\n",
    "\n",
    "        p_list=['train','valid','test']\n",
    "        for p in p_list:\n",
    "            \n",
    "            p_true=locals()['y_'+str(p)+'_true']\n",
    "            p_prob=locals()['y_prob_'+str(p)]\n",
    "            p_pred=locals()['y_'+str(p)+'_pred']\n",
    "\n",
    "            locals()[str(p)+'_MSE'] =metrics.mean_squared_error(p_true,p_pred).round(3) \n",
    "            locals()[str(p)+'_RMSE'] =metrics.mean_squared_error(p_true, p_pred, squared=True).round(3)   \n",
    "            locals()[str(p)+'_MAE']=metrics.mean_absolute_error(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_R2'] =metrics.r2_score(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_Pearsonr'] =scipy.stats.pearsonr(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_Median_AE'] =metrics.median_absolute_error(y_train_true, y_train_pred).round(3)\n",
    "\t\n",
    "            result_list.append(locals()[str(p)+'_MSE'])\n",
    "            result_list.append(locals()[str(p)+'_RMSE'])\n",
    "            result_list.append(locals()[ str(p)+'_MAE'])    \n",
    "            result_list.append(locals()[str(p)+'_R2'])\n",
    "            result_list.append(locals()[str(p)+'_Pearsonr'])\n",
    "            result_list.append(locals()[str(p)+'_Median_AE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "707cfb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3#\n",
    "def evaluation_NF(labels,problem_mode,i):   \n",
    "    t_v=['train','valid','test']\n",
    "    ###以第0層df，當作基底增加每層特徵，索以第0層保留###\n",
    "    for tv in t_v:\n",
    "        print('------------'+str(tv)+'-------------')\n",
    "        for Task_name in tqdm(labels):\n",
    "            predict_feature=(globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i)]).drop(columns=[Task_name])\n",
    "            globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i+1)]=globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(0)].copy()\n",
    "\n",
    "            for Task_test in range(len(labels)): \n",
    "                if problem_mode=='classification':\n",
    "                    prob= globals()['predictor_'+str(labels[Task_test])+'_'+str(i)].predict_proba(predict_feature)[:, 1]\n",
    "                else:\n",
    "                    prob= globals()['predictor_'+str(labels[Task_test])+'_'+str(i)].predict(predict_feature)\n",
    "\n",
    "                f = 'new_feature_layer_'+str(i)+'_feature_by_'+str(labels[Task_test])\n",
    "                globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i+1)].loc[:,f]=prob.copy()                       \n",
    "                globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i+1)]=globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i+1)].copy()\n",
    "                globals()['Task_check'+str(Task_name)+'_'+str(tv)+'_'+str(i+1)]=globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i+1)].copy()\n",
    "\n",
    "       ##釋出不需要的variable##\n",
    "    if i!=0:\n",
    "        for tv in t_v:\n",
    "            print('----釋出variable----'+str(tv)+'-------------')\n",
    "            for Task_name in tqdm(labels):\n",
    "                del globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba71687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4#\n",
    "def raw2mean(file,problem_mode,main_perform,save_file_name):\n",
    "    file['Layer']=file['Layer'].astype(int)\n",
    "    file=file.dropna()\n",
    "\n",
    "    number = list(range(0,500))\n",
    "    result_mean_df=pd.DataFrame(index=number )\n",
    "\n",
    "    regression_col_list=['Layer' , \n",
    "              'Train_MSE','Train_RMSE','Train_MAE','Train_R2','Train_Pearsonr','Train_Median_AE',            \n",
    "            'Validation_MSE','Validation_RMSE','Validation_MAE','Validation_R2','Validation_Pearsonr','Validation_Median_AE',            \n",
    "            'Test_MSE','Test_RMSE','Test_MAE','Test_R2','Test_Pearsonr','Test_Median_AE']            \n",
    "\n",
    "    classification_col_list=['Layer',  \n",
    "            'Train AUC', 'Train ACC','Train Balance ACC','Train F1','Train Precision', 'Train Recall','Train Specificity',\n",
    "            'Validation AUC','Validation ACC', 'Validation Balance ACC','Validation F1','Validation Precision','Validation Recall','Validation Specificity',\n",
    "              'Test_AUC', 'Test ACC', 'Test Balance ACC','Test F1',\t 'Test Precision','Test Recall','Test Specificity']\n",
    "    \n",
    "    if problem_mode=='classification':\n",
    "        col_list=classification_col_list\n",
    "    else:\n",
    "        col_list=regression_col_list\n",
    "    \n",
    "    col_list=classification_col_list\n",
    "    col_list.append('#Mean>0.8')\n",
    "    \n",
    "    file_layer=file['Layer'].unique()\n",
    "    file_layer=file_layer.tolist()\n",
    "    mean_list=locals \n",
    "    ###各層平均###\n",
    "    a=0\n",
    "    for i in file_layer:\n",
    "\n",
    "        file_layer=file['Layer'].unique()\n",
    "\n",
    "        layer_len=len(file_layer)\n",
    "        file_layer=file[file['Layer']==i]\n",
    "        if problem_mode=='classification':\n",
    "            file_class=file_layer.iloc[:,2:23]\n",
    "        else:\n",
    "            file_class=file_layer.iloc[:,2:20]            \n",
    "                         \n",
    "        mean_list=file_class.mean()\n",
    "        mean_list=mean_list.round(3)\n",
    "        mean_list=list(mean_list)\n",
    "        task_number=file_class[file_class['Test_'+str(main_perform)]>=0.8]\n",
    "        noft=list(task_number['Test_'+str(main_perform)])\n",
    "        noft=len(noft)       \n",
    "        mean_list.insert(0,i)\n",
    "        mean_list.append(noft)\n",
    "        for r in range(len(col_list)):\n",
    "            result_mean_df.loc[a,col_list[r]]=mean_list[r]\n",
    "        a=a+1\n",
    "\n",
    "    result_mean_df=result_mean_df.dropna()    \n",
    "    result_mean_df.to_csv('MTRFNET_'+str(save_file_name)+'_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34169e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "任務型態:classification\n",
      "評估指標:AUC\n",
      "-----preprocessing-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----training----0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/48 [00:03<02:29,  3.18s/it]\u001b[A\n",
      "  4%|▍         | 2/48 [00:06<02:22,  3.09s/it]\u001b[A\n",
      "  6%|▋         | 3/48 [00:09<02:20,  3.12s/it]\u001b[A\n",
      "  8%|▊         | 4/48 [00:12<02:17,  3.13s/it]\u001b[A\n",
      " 10%|█         | 5/48 [00:15<02:15,  3.16s/it]\u001b[A\n",
      " 12%|█▎        | 6/48 [00:18<02:12,  3.14s/it]\u001b[A\n",
      " 15%|█▍        | 7/48 [00:21<02:08,  3.12s/it]\u001b[A\n",
      " 17%|█▋        | 8/48 [00:25<02:04,  3.12s/it]\u001b[A\n",
      " 19%|█▉        | 9/48 [00:28<02:01,  3.11s/it]\u001b[A\n",
      " 21%|██        | 10/48 [00:31<01:57,  3.10s/it]\u001b[A\n",
      " 23%|██▎       | 11/48 [00:34<01:55,  3.13s/it]\u001b[A\n",
      " 25%|██▌       | 12/48 [00:37<01:53,  3.14s/it]\u001b[A\n",
      " 27%|██▋       | 13/48 [00:40<01:49,  3.11s/it]\u001b[A\n",
      " 29%|██▉       | 14/48 [00:43<01:46,  3.12s/it]\u001b[A\n",
      " 31%|███▏      | 15/48 [00:46<01:44,  3.15s/it]\u001b[A\n",
      " 33%|███▎      | 16/48 [00:50<01:40,  3.14s/it]\u001b[A\n",
      " 35%|███▌      | 17/48 [00:53<01:37,  3.14s/it]\u001b[A\n",
      " 38%|███▊      | 18/48 [00:56<01:33,  3.13s/it]\u001b[A\n",
      " 40%|███▉      | 19/48 [00:59<01:30,  3.13s/it]\u001b[A\n",
      " 42%|████▏     | 20/48 [01:02<01:28,  3.15s/it]\u001b[A\n",
      " 44%|████▍     | 21/48 [01:05<01:24,  3.15s/it]\u001b[A\n",
      " 46%|████▌     | 22/48 [01:08<01:19,  3.05s/it]\u001b[A\n",
      " 48%|████▊     | 23/48 [01:11<01:16,  3.08s/it]\u001b[A\n",
      " 50%|█████     | 24/48 [01:14<01:14,  3.11s/it]\u001b[A\n",
      " 52%|█████▏    | 25/48 [01:18<01:11,  3.13s/it]\u001b[A\n",
      " 54%|█████▍    | 26/48 [01:21<01:09,  3.14s/it]\u001b[A\n",
      " 56%|█████▋    | 27/48 [01:24<01:05,  3.12s/it]\u001b[A\n",
      " 58%|█████▊    | 28/48 [01:27<01:02,  3.12s/it]\u001b[A\n",
      " 60%|██████    | 29/48 [01:30<00:59,  3.12s/it]\u001b[A\n",
      " 62%|██████▎   | 30/48 [01:33<00:56,  3.12s/it]\u001b[A\n",
      " 65%|██████▍   | 31/48 [01:36<00:53,  3.14s/it]\u001b[A\n",
      " 67%|██████▋   | 32/48 [01:39<00:49,  3.12s/it]\u001b[A\n",
      " 69%|██████▉   | 33/48 [01:43<00:46,  3.12s/it]\u001b[A\n",
      " 71%|███████   | 34/48 [01:46<00:43,  3.13s/it]\u001b[A\n",
      " 73%|███████▎  | 35/48 [01:49<00:40,  3.14s/it]\u001b[A\n",
      " 75%|███████▌  | 36/48 [01:52<00:37,  3.15s/it]\u001b[A\n",
      " 77%|███████▋  | 37/48 [01:55<00:34,  3.15s/it]\u001b[A\n",
      " 79%|███████▉  | 38/48 [01:58<00:31,  3.15s/it]\u001b[A\n",
      " 81%|████████▏ | 39/48 [02:01<00:28,  3.14s/it]\u001b[A\n",
      " 83%|████████▎ | 40/48 [02:05<00:25,  3.15s/it]\u001b[A\n",
      " 85%|████████▌ | 41/48 [02:08<00:21,  3.14s/it]\u001b[A\n",
      " 88%|████████▊ | 42/48 [02:11<00:18,  3.14s/it]\u001b[A\n",
      " 90%|████████▉ | 43/48 [02:14<00:15,  3.15s/it]\u001b[A\n",
      " 92%|█████████▏| 44/48 [02:17<00:12,  3.13s/it]\u001b[A\n",
      " 94%|█████████▍| 45/48 [02:20<00:09,  3.13s/it]\u001b[A\n",
      " 96%|█████████▌| 46/48 [02:23<00:06,  3.13s/it]\u001b[A\n",
      " 98%|█████████▊| 47/48 [02:27<00:03,  3.24s/it]\u001b[A\n",
      "100%|██████████| 48/48 [02:29<00:00,  3.12s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------train-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/48 [00:13<10:24, 13.29s/it]\u001b[A\n",
      "  4%|▍         | 2/48 [00:26<10:08, 13.23s/it]\u001b[A\n",
      "  6%|▋         | 3/48 [00:39<09:52, 13.17s/it]\u001b[A\n",
      "  8%|▊         | 4/48 [00:52<09:40, 13.19s/it]\u001b[A\n",
      " 10%|█         | 5/48 [01:06<09:28, 13.23s/it]\u001b[A\n",
      " 12%|█▎        | 6/48 [01:20<09:37, 13.74s/it]\u001b[A\n",
      " 15%|█▍        | 7/48 [01:34<09:16, 13.56s/it]\u001b[A\n",
      " 17%|█▋        | 8/48 [01:47<08:58, 13.46s/it]\u001b[A\n",
      " 19%|█▉        | 9/48 [02:00<08:42, 13.39s/it]\u001b[A\n",
      " 21%|██        | 10/48 [02:13<08:26, 13.32s/it]\u001b[A\n",
      " 23%|██▎       | 11/48 [02:26<08:12, 13.30s/it]\u001b[A\n",
      " 25%|██▌       | 12/48 [02:40<07:58, 13.30s/it]\u001b[A\n",
      " 27%|██▋       | 13/48 [02:53<07:44, 13.28s/it]\u001b[A\n",
      " 29%|██▉       | 14/48 [03:06<07:30, 13.26s/it]\u001b[A\n",
      " 31%|███▏      | 15/48 [03:19<07:18, 13.27s/it]\u001b[A\n",
      " 33%|███▎      | 16/48 [03:33<07:03, 13.24s/it]\u001b[A\n",
      " 35%|███▌      | 17/48 [03:46<06:50, 13.25s/it]\u001b[A\n",
      " 38%|███▊      | 18/48 [03:59<06:38, 13.27s/it]\u001b[A\n",
      " 40%|███▉      | 19/48 [04:12<06:23, 13.23s/it]\u001b[A\n",
      " 42%|████▏     | 20/48 [04:26<06:10, 13.22s/it]\u001b[A\n",
      " 44%|████▍     | 21/48 [04:39<05:56, 13.21s/it]\u001b[A\n",
      " 46%|████▌     | 22/48 [04:49<05:19, 12.28s/it]\u001b[A\n",
      " 48%|████▊     | 23/48 [05:02<05:13, 12.55s/it]\u001b[A\n",
      " 50%|█████     | 24/48 [05:15<05:05, 12.74s/it]\u001b[A\n",
      " 52%|█████▏    | 25/48 [05:28<04:56, 12.89s/it]\u001b[A\n",
      " 54%|█████▍    | 26/48 [05:42<04:46, 13.02s/it]\u001b[A\n",
      " 56%|█████▋    | 27/48 [05:55<04:35, 13.10s/it]\u001b[A\n",
      " 58%|█████▊    | 28/48 [06:08<04:23, 13.17s/it]\u001b[A\n",
      " 60%|██████    | 29/48 [06:22<04:10, 13.18s/it]\u001b[A\n",
      " 62%|██████▎   | 30/48 [06:35<03:57, 13.19s/it]\u001b[A\n",
      " 65%|██████▍   | 31/48 [06:48<03:44, 13.21s/it]\u001b[A\n",
      " 67%|██████▋   | 32/48 [07:01<03:31, 13.25s/it]\u001b[A\n",
      " 69%|██████▉   | 33/48 [07:15<03:18, 13.22s/it]\u001b[A\n",
      " 71%|███████   | 34/48 [07:28<03:05, 13.23s/it]\u001b[A\n",
      " 73%|███████▎  | 35/48 [07:41<02:51, 13.23s/it]\u001b[A\n",
      " 75%|███████▌  | 36/48 [07:54<02:38, 13.21s/it]\u001b[A\n",
      " 77%|███████▋  | 37/48 [08:07<02:25, 13.23s/it]\u001b[A\n",
      " 79%|███████▉  | 38/48 [08:21<02:12, 13.24s/it]\u001b[A\n",
      " 81%|████████▏ | 39/48 [08:34<01:59, 13.29s/it]\u001b[A\n",
      " 83%|████████▎ | 40/48 [08:47<01:46, 13.25s/it]\u001b[A\n",
      " 85%|████████▌ | 41/48 [09:01<01:32, 13.26s/it]\u001b[A\n",
      " 88%|████████▊ | 42/48 [09:14<01:19, 13.30s/it]\u001b[A\n",
      " 90%|████████▉ | 43/48 [09:27<01:06, 13.28s/it]\u001b[A\n",
      " 92%|█████████▏| 44/48 [09:40<00:52, 13.25s/it]\u001b[A\n",
      " 94%|█████████▍| 45/48 [09:54<00:39, 13.25s/it]\u001b[A\n",
      " 96%|█████████▌| 46/48 [10:07<00:26, 13.31s/it]\u001b[A\n",
      " 98%|█████████▊| 47/48 [10:22<00:13, 13.82s/it]\u001b[A\n",
      "100%|██████████| 48/48 [10:32<00:00, 13.17s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------valid-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/48 [00:09<07:41,  9.81s/it]\u001b[A\n",
      "  4%|▍         | 2/48 [00:19<07:28,  9.75s/it]\u001b[A\n",
      "  6%|▋         | 3/48 [00:29<07:29,  9.98s/it]\u001b[A\n",
      "  8%|▊         | 4/48 [00:39<07:16,  9.92s/it]\u001b[A\n",
      " 10%|█         | 5/48 [00:49<07:02,  9.83s/it]\u001b[A\n",
      " 12%|█▎        | 6/48 [00:59<06:53,  9.85s/it]\u001b[A\n",
      " 15%|█▍        | 7/48 [01:08<06:40,  9.78s/it]\u001b[A\n",
      " 17%|█▋        | 8/48 [01:18<06:31,  9.78s/it]\u001b[A\n",
      " 19%|█▉        | 9/48 [01:28<06:23,  9.82s/it]\u001b[A\n",
      " 21%|██        | 10/48 [01:38<06:16,  9.90s/it]\u001b[A\n",
      " 23%|██▎       | 11/48 [01:48<06:07,  9.92s/it]\u001b[A\n",
      " 25%|██▌       | 12/48 [01:58<05:57,  9.94s/it]\u001b[A\n",
      " 27%|██▋       | 13/48 [02:08<05:47,  9.92s/it]\u001b[A\n",
      " 29%|██▉       | 14/48 [02:18<05:37,  9.93s/it]\u001b[A\n",
      " 31%|███▏      | 15/48 [02:28<05:29,  9.97s/it]\u001b[A\n",
      " 33%|███▎      | 16/48 [02:38<05:16,  9.89s/it]\u001b[A\n",
      " 35%|███▌      | 17/48 [02:47<05:06,  9.88s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from resource import getrusage, RUSAGE_SELF\n",
    "\n",
    "##########################################################\n",
    "##1 random seed##\n",
    "global seed\n",
    "seed=8 #Ex:8\n",
    "\n",
    "##2 檔案路徑##\n",
    "#data_path=('')#Ex:'Zebrafish_Tasks_Dataset.csv'#\n",
    "data_path=('Zebrafish_final202310_withMorphologyALLdata_ECFP6.csv')\n",
    "\n",
    "##3 所有任務名稱##\n",
    "#task_list=[]#Ex:['Label_name1','Label_name2','Label_name3'....]#\n",
    "\n",
    "task_list=['LEL_MORT','LEL_YSE','LEL_AXIS','LEL_EYE','LEL_SNOU', 'LEL_JAW',\n",
    "'LEL_OTIC','LEL_PE','LEL_BRAI','LEL_SOMI','LEL_PFIN','LEL_CFIN',\n",
    "'LEL_PIG','LEL_CIRC', 'LEL_TRUN','LEL_SWIM','LEL_NC','LEL_TR', '18_END_LEC', 'MOR_LEC','SUBLETH_17_END_LEC', 'TOX_SCO',\n",
    "'MO24', 'DP24', 'SM24', 'NC24','MORT','YSE', 'AXIS', 'EYE',\n",
    "'SNOU', 'JAW', 'OTIC', 'PE', 'BRAI', 'SOMI', 'PFIN', 'CFIN',\n",
    "'PIG','CIRC', 'TRUN', 'SWIM','NC','TR','MOV21', 'AUC21','METAB','MIC']\n",
    "\n",
    "##4 結果儲存檔案名稱##\n",
    "save_file_name='test' #Ex:'pk'#\n",
    "\n",
    "###5 ####\n",
    "problem_mode='classification'\n",
    "#classification,regression#\n",
    "\n",
    "###6主要評估指標####\n",
    "main_perform='AUC'\n",
    "# 'AUC', ' ACC',' Balance ACC',' F1',' Precision', ' Recall',' Specificity','MSE','RMSE','MAE','R2','Pearsonr','Median_AE'#         \n",
    "\n",
    "##以下不用修改##\n",
    "import time\n",
    "start=time.time()\n",
    "print('任務型態:'+str(problem_mode))\n",
    "print('評估指標:'+str(main_perform))\n",
    "MAIN_RF_MT(data_path,task_list,seed,problem_mode,main_perform,save_file_name)\n",
    "end=time.time()\n",
    "print('執行時間: ',end - start)\n",
    "print(\"peak memory:\", getrusage(RUSAGE_SELF).ru_maxrss / 1000 / 1000, \"MB\")\n",
    "\n",
    "#目前使用的參數#\n",
    "#資料比例:7:1:2\n",
    "#Validation:Independet\n",
    "#RF:n_estimators=500,max_features='log2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3c636-462d-4664-853a-9ef00982a505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
