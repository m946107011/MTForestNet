{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN#\n",
    "def MAIN_RF_MT(data_path,task_list,seed,problem_mode,main_perform,save_file_name):\n",
    "    read_preprocess(data_path,task_list,seed)\n",
    "\n",
    "    q=0\n",
    "    number = list(range(0,500))    \n",
    "    result_df = locals\n",
    "    result_df=pd.DataFrame(index=number)   \n",
    "    \n",
    "    regression_col_list=['Layer','Task' , \n",
    "              'Train_MSE','Train_RMSE','Train_MAE','Train_R2','Train_Pearsonr','Train_Median_AE',            \n",
    "            'Validation_MSE','Validation_RMSE','Validation_MAE','Validation_R2','Validation_Pearsonr','Validation_Median_AE',            \n",
    "            'Test_MSE','Test_RMSE','Test_MAE','Test_R2','Test_Pearsonr','Test_Median_AE']            \n",
    "\n",
    "    classification_col_list=['Layer',  'Task',\n",
    "            'Train AUC', 'Train ACC','Train Balance ACC','Train F1','Train Precision', 'Train Sensitivity','Train Specificity',\n",
    "            'Validation AUC','Validation ACC', 'Validation Balance ACC','Validation F1','Validation Precision','Validation Sensitivity','Validation Specificity',\n",
    "              'Test_AUC', 'Test ACC', 'Test Balance ACC','Test F1',\t 'Test Precision','Test Sensitivity','Test Specificity']\n",
    "    \n",
    "    if problem_mode=='classification':\n",
    "        col_list=classification_col_list\n",
    "    else:\n",
    "        col_list=regression_col_list\n",
    "\n",
    "    decision_df=pd.DataFrame()\n",
    "    i=0\n",
    "    for i in tqdm(range(0,10)):        \n",
    "        if decision_df.empty :\n",
    "            print('----training----'+str(i))  \n",
    "            for label in tqdm(labels):\n",
    "                train_perform(label,i,problem_mode,seed)\n",
    "                for r in range(len(col_list)):\n",
    "                    result_df.at[q,col_list[r]]=result_list[r]\n",
    "                result_df=result_df.round(3)   \n",
    "                q=q+1\n",
    "                \n",
    "            ####修改評估條件######\n",
    "            result_df['Validation '+str(main_perform)]=result_df['Validation '+str(main_perform)].abs()\n",
    "            validation_df_for_mean=result_df.loc[(result_df['Layer']==i)]                   \n",
    "            validation_mean=validation_df_for_mean['Validation '+str(main_perform)].mean(axis=0)\n",
    "            result_df.loc[(result_df['Layer']==i),'Mean '+str(main_perform)]=validation_mean            \n",
    "            result_df['Mean '+str(main_perform)]=result_df['Mean '+str(main_perform)].round(3)\n",
    "                \n",
    "            if main_perform =='AUC' or 'ACC' or 'Balance ACC' or 'F1' or 'Precision' or'Recall'or'Specificity'or'R2'or'Pearsonr':\n",
    "                if (i== 0) or (list(set(result_df.iloc[list(result_df.Layer== i-1) ,-1]))<list(set(result_df.iloc[list(result_df.Layer== i) ,-1]))):        \n",
    "                    evaluation_NF(labels,problem_mode,i)\n",
    "                else:\n",
    "                    print('開始存出Excel')\n",
    "                    result_df=result_df.round(3)\n",
    "                    result_df=result_df.dropna()\n",
    "\n",
    "                    result_df.to_csv('MTRFNET_'+str(save_file_name)+'_result.csv')\n",
    "                    decision_df=result_df\n",
    "                    #count average performance#\n",
    "                    file=result_df\n",
    "                    raw2mean(file,problem_mode,main_perform,save_file_name)                    \n",
    "\n",
    "            else:\n",
    "                if (i== 0) or (list(set(result_df.iloc[list(result_df.Layer== i-1) ,-1]))>list(set(result_df.iloc[list(result_df.Layer== i) ,-1]))):        \n",
    "                    evaluation_NF(labels,problem_mode,i)\n",
    "\n",
    "                else:\n",
    "                    print('開始存出Excel')\n",
    "                    result_df=result_df.round(3)\n",
    "                    result_df=result_df.dropna()\n",
    "\n",
    "                    result_df.to_csv('MTRFNET_'+str(save_file_name)+'_result.csv')\n",
    "                    decision_df=result_df\n",
    "                    #count average performance#\n",
    "                    file=result_df\n",
    "                    raw2mean(file,problem_mode,main_perform,save_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f45b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1#\n",
    "def read_preprocess(data_path,task_list,seed):\n",
    "    print('-----preprocessing-----')  \n",
    "    data=pd.read_csv(data_path)   \n",
    "    if 'Unnamed: 0' in data:\n",
    "        data=data.drop(columns=['Unnamed: 0'])\n",
    "    if 'SMILES' in data:\n",
    "        data=data.drop(columns=['SMILES'])\n",
    "    if 'smiles' in data:\n",
    "        data=data.drop(columns=['smiles'])    \n",
    "    if 'mol_id' in data:\n",
    "        data=data.drop(columns=['mol_id'])    \n",
    "\n",
    "###task_list###                   \n",
    "    global labels\n",
    "    labels=task_list\n",
    "\n",
    "    for i in data.index:\n",
    "        for q in labels:\n",
    "            try:\n",
    "                if data.loc[i,q]== 'ACT':\n",
    "                    data.loc[i,q]=int(1)\n",
    "                if data.loc[i,q]== 'INACT':           \n",
    "                    data.loc[i,q]=int(0)             \n",
    "            except:\n",
    "                pass   \n",
    "    for label in labels:\n",
    "        labels_to_drop=[l for l in labels if l !=label]\n",
    "        locals()['Task_'+str(label)]  = data.drop(labels_to_drop,axis=1)  \n",
    "        locals()['Task_'+str(label)]= locals()['Task_'+str(label)].dropna(subset=[label])\n",
    "    for label in labels: \n",
    "        globals()['Task_'+str(label)+'_train_0'],globals()['Task_'+str(label)+'_test_0'] = train_test_split(locals()['Task_'+str(label)], test_size=0.2,random_state=seed ,stratify=locals()['Task_'+str(label)][label])\n",
    "        globals()['Task_'+str(label)+'_train_0'],globals()['Task_'+str(label)+'_valid_0'] = train_test_split(globals()['Task_'+str(label)+'_train_0'], test_size=0.125,random_state=seed ,stratify=globals()['Task_'+str(label)+'_train_0'][label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91928c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2#\n",
    "def train_perform(label,i,problem_mode,seed):    \n",
    "    \n",
    "    y_train=globals()['Task_'+str(label)+'_train_'+str(i)][label].astype('float')\n",
    "    x_train=globals()['Task_'+str(label)+'_train_'+str(i)].drop(columns=label)    \n",
    "    y_valid=globals()['Task_'+str(label)+'_valid_'+str(i)][label].astype('float')\n",
    "    x_valid=globals()['Task_'+str(label)+'_valid_'+str(i)].drop(columns=label)   \n",
    "    y_test=globals()['Task_'+str(label)+'_test_'+str(i)][label].astype('float')\n",
    "    x_test=globals()['Task_'+str(label)+'_test_'+str(i)].drop(columns=label)    \n",
    "    \n",
    "    ##model_parameter##\n",
    "    if problem_mode=='classification':\n",
    "        model_parameter=RandomForestClassifier(n_estimators=500,max_features='log2',random_state=seed,n_jobs=-2)\n",
    "    else:\n",
    "        model_parameter=RandomForestRegressor(n_estimators=500,max_features='log2',random_state=seed,n_jobs=-2)\n",
    "           \n",
    "    x_train=x_train.astype(float)\n",
    "    y_train=y_train.astype(float)    \n",
    "\n",
    "    ##model##\n",
    "    globals()['predictor_'+str(label)+'_'+str(i)]=model_parameter.fit(x_train, y_train)\n",
    "    ###儲存模型###\n",
    "    #joblib.dump(globals()['predictor_'+str(label)+'_'+str(i)], './Zebrafish_Models_withALLMorphology_ECFP6_202310_prob/predictor_'+str(label)+'_'+str(i))\n",
    "\n",
    "    ###    \n",
    "    locals()['y_train_true']=(y_train)\n",
    "    locals()['y_valid_true']=(y_valid)\n",
    "    locals()['y_test_true']=(y_test)\n",
    "\n",
    "    locals()['y_train_pred']=globals()['predictor_'+str(label)+'_'+str(i)].predict(x_train)\n",
    "    locals()['y_valid_pred']=globals()['predictor_'+str(label)+'_'+str(i)].predict(x_valid)\n",
    "    locals()['y_test_pred']=globals()['predictor_'+str(label)+'_'+str(i)].predict(x_test)\n",
    "\n",
    "    locals()['y_prob_train']=globals()['predictor_'+str(label)+'_'+str(i)].predict_proba(x_train)[:, 1]\n",
    "    locals()['y_prob_test']=globals()['predictor_'+str(label)+'_'+str(i)].predict_proba(x_test)[:, 1]\n",
    "    locals()['y_prob_valid']=globals()['predictor_'+str(label)+'_'+str(i)].predict_proba(x_valid)[:, 1]\n",
    "\n",
    "    ##train_performances##\n",
    "    global result_list\n",
    "    result_list=[]\n",
    "    result_list.append(int(i))\n",
    "    result_list.append(label)\n",
    "    \n",
    "    if problem_mode == 'classification':\n",
    "        \n",
    "        p_list=['train','valid','test']\n",
    "        for p in p_list:\n",
    "            \n",
    "            p_true=locals()['y_'+str(p)+'_true']\n",
    "            p_prob=locals()['y_prob_'+str(p)]\n",
    "            p_pred=locals()['y_'+str(p)+'_pred']\n",
    "            \n",
    "            locals()[str(p)+'_AUC'] =metrics.roc_auc_score(p_true,p_prob).round(3) \n",
    "            locals()[str(p)+'_ACC'] =metrics.accuracy_score(p_true, p_pred).round(3)   \n",
    "            locals()[str(p)+'_BACC']=metrics.balanced_accuracy_score(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_F1'] =metrics.f1_score(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_PR'] =metrics.precision_score(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_R']=metrics.recall_score(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_S'] =metrics.recall_score(p_true, p_pred, pos_label=0).round(3)\n",
    "\n",
    "            result_list.append(locals()[str(p)+'_AUC'])\n",
    "            result_list.append(locals()[str(p)+'_ACC'])\n",
    "            result_list.append(locals()[str(p)+'_BACC'])    \n",
    "            result_list.append(locals()[str(p)+'_F1'])\n",
    "            result_list.append(locals()[str(p)+'_PR'])\n",
    "            result_list.append(locals()[str(p)+'_R'])\n",
    "            result_list.append(locals()[str(p)+'_S'])        \n",
    "    else:\n",
    "\n",
    "        p_list=['train','valid','test']\n",
    "        for p in p_list:\n",
    "            \n",
    "            p_true=locals()['y_'+str(p)+'_true']\n",
    "            p_prob=locals()['y_prob_'+str(p)]\n",
    "            p_pred=locals()['y_'+str(p)+'_pred']\n",
    "\n",
    "            locals()[str(p)+'_MSE'] =metrics.mean_squared_error(p_true,p_pred).round(3) \n",
    "            locals()[str(p)+'_RMSE'] =metrics.mean_squared_error(p_true, p_pred, squared=True).round(3)   \n",
    "            locals()[str(p)+'_MAE']=metrics.mean_absolute_error(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_R2'] =metrics.r2_score(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_Pearsonr'] =scipy.stats.pearsonr(p_true, p_pred).round(3)\n",
    "            locals()[str(p)+'_Median_AE'] =metrics.median_absolute_error(y_train_true, y_train_pred).round(3)\n",
    "\t\n",
    "            result_list.append(locals()[str(p)+'_MSE'])\n",
    "            result_list.append(locals()[str(p)+'_RMSE'])\n",
    "            result_list.append(locals()[ str(p)+'_MAE'])    \n",
    "            result_list.append(locals()[str(p)+'_R2'])\n",
    "            result_list.append(locals()[str(p)+'_Pearsonr'])\n",
    "            result_list.append(locals()[str(p)+'_Median_AE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707cfb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3#\n",
    "def evaluation_NF(labels,problem_mode,i):   \n",
    "    t_v=['train','valid','test']\n",
    "    ###以第0層df，當作基底增加每層特徵，索以第0層保留###\n",
    "    for tv in t_v:\n",
    "        print('------------'+str(tv)+'-------------')\n",
    "        for Task_name in tqdm(labels):\n",
    "            predict_feature=(globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i)]).drop(columns=[Task_name])\n",
    "            globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i+1)]=globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(0)].copy()\n",
    "\n",
    "            for Task_test in range(len(labels)): \n",
    "                if problem_mode=='classification':\n",
    "                    prob= globals()['predictor_'+str(labels[Task_test])+'_'+str(i)].predict_proba(predict_feature)[:, 1]\n",
    "                else:\n",
    "                    prob= globals()['predictor_'+str(labels[Task_test])+'_'+str(i)].predict(predict_feature)\n",
    "\n",
    "                f = 'new_feature_layer_'+str(i)+'_feature_by_'+str(labels[Task_test])\n",
    "                globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i+1)].loc[:,f]=prob.copy()                       \n",
    "                globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i+1)]=globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i+1)].copy()\n",
    "                globals()['Task_check'+str(Task_name)+'_'+str(tv)+'_'+str(i+1)]=globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i+1)].copy()\n",
    "\n",
    "       ##釋出不需要的variable##\n",
    "    if i!=0:\n",
    "        for tv in t_v:\n",
    "            print('----釋出variable----'+str(tv)+'-------------')\n",
    "            for Task_name in tqdm(labels):\n",
    "                del globals()['Task_'+str(Task_name)+'_'+str(tv)+'_'+str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba71687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4#\n",
    "def raw2mean(file,problem_mode,main_perform,save_file_name):\n",
    "    file['Layer']=file['Layer'].astype(int)\n",
    "    file=file.dropna()\n",
    "\n",
    "    number = list(range(0,500))\n",
    "    result_mean_df=pd.DataFrame(index=number )\n",
    "\n",
    "    regression_col_list=['Layer' , \n",
    "              'Train_MSE','Train_RMSE','Train_MAE','Train_R2','Train_Pearsonr','Train_Median_AE',            \n",
    "            'Validation_MSE','Validation_RMSE','Validation_MAE','Validation_R2','Validation_Pearsonr','Validation_Median_AE',            \n",
    "            'Test_MSE','Test_RMSE','Test_MAE','Test_R2','Test_Pearsonr','Test_Median_AE']            \n",
    "\n",
    "    classification_col_list=['Layer',  \n",
    "            'Train AUC', 'Train ACC','Train Balance ACC','Train F1','Train Precision', 'Train Recall','Train Specificity',\n",
    "            'Validation AUC','Validation ACC', 'Validation Balance ACC','Validation F1','Validation Precision','Validation Recall','Validation Specificity',\n",
    "              'Test_AUC', 'Test ACC', 'Test Balance ACC','Test F1',\t 'Test Precision','Test Recall','Test Specificity']\n",
    "    \n",
    "    if problem_mode=='classification':\n",
    "        col_list=classification_col_list\n",
    "    else:\n",
    "        col_list=regression_col_list\n",
    "    \n",
    "    col_list=classification_col_list\n",
    "    col_list.append('#Mean>0.8')\n",
    "    \n",
    "    file_layer=file['Layer'].unique()\n",
    "    file_layer=file_layer.tolist()\n",
    "    mean_list=locals \n",
    "    ###各層平均###\n",
    "    a=0\n",
    "    for i in file_layer:\n",
    "\n",
    "        file_layer=file['Layer'].unique()\n",
    "\n",
    "        layer_len=len(file_layer)\n",
    "        file_layer=file[file['Layer']==i]\n",
    "        if problem_mode=='classification':\n",
    "            file_class=file_layer.iloc[:,2:23]\n",
    "        else:\n",
    "            file_class=file_layer.iloc[:,2:20]            \n",
    "                         \n",
    "        mean_list=file_class.mean()\n",
    "        mean_list=mean_list.round(3)\n",
    "        mean_list=list(mean_list)\n",
    "        task_number=file_class[file_class['Test_'+str(main_perform)]>=0.8]\n",
    "        noft=list(task_number['Test_'+str(main_perform)])\n",
    "        noft=len(noft)       \n",
    "        mean_list.insert(0,i)\n",
    "        mean_list.append(noft)\n",
    "        for r in range(len(col_list)):\n",
    "            result_mean_df.loc[a,col_list[r]]=mean_list[r]\n",
    "        a=a+1\n",
    "\n",
    "    result_mean_df=result_mean_df.dropna()    \n",
    "    result_mean_df.to_csv('MTRFNET_'+str(save_file_name)+'_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34169e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from resource import getrusage, RUSAGE_SELF\n",
    "\n",
    "##########################################################\n",
    "##1 random seed##\n",
    "global seed\n",
    "seed=8 #Ex:8\n",
    "\n",
    "##2 檔案路徑##\n",
    "#data_path=('')#Ex:'Zebrafish_Tasks_Dataset.csv'#\n",
    "data_path=('Zebrafish_final202310_withMorphologyALLdata_ECFP6.csv')\n",
    "\n",
    "##3 所有任務名稱##\n",
    "#task_list=[]#Ex:['Label_name1','Label_name2','Label_name3'....]#\n",
    "\n",
    "task_list=['LEL_MORT','LEL_YSE','LEL_AXIS','LEL_EYE','LEL_SNOU', 'LEL_JAW',\n",
    "'LEL_OTIC','LEL_PE','LEL_BRAI','LEL_SOMI','LEL_PFIN','LEL_CFIN',\n",
    "'LEL_PIG','LEL_CIRC', 'LEL_TRUN','LEL_SWIM','LEL_NC','LEL_TR', '18_END_LEC', 'MOR_LEC','SUBLETH_17_END_LEC', 'TOX_SCO',\n",
    "'MO24', 'DP24', 'SM24', 'NC24','MORT','YSE', 'AXIS', 'EYE',\n",
    "'SNOU', 'JAW', 'OTIC', 'PE', 'BRAI', 'SOMI', 'PFIN', 'CFIN',\n",
    "'PIG','CIRC', 'TRUN', 'SWIM','NC','TR','MOV21', 'AUC21','METAB','MIC']\n",
    "\n",
    "##4 結果儲存檔案名稱##\n",
    "save_file_name='test' #Ex:'pk'#\n",
    "\n",
    "###5 ####\n",
    "problem_mode='classification'\n",
    "#classification,regression#\n",
    "\n",
    "###6主要評估指標####\n",
    "main_perform='AUC'\n",
    "# 'AUC', ' ACC',' Balance ACC',' F1',' Precision', ' Recall',' Specificity','MSE','RMSE','MAE','R2','Pearsonr','Median_AE'#         \n",
    "\n",
    "##以下不用修改##\n",
    "import time\n",
    "start=time.time()\n",
    "print('任務型態:'+str(problem_mode))\n",
    "print('評估指標:'+str(main_perform))\n",
    "MAIN_RF_MT(data_path,task_list,seed,problem_mode,main_perform,save_file_name)\n",
    "end=time.time()\n",
    "print('執行時間: ',end - start)\n",
    "print(\"peak memory:\", getrusage(RUSAGE_SELF).ru_maxrss / 1000 / 1000, \"MB\")\n",
    "\n",
    "#目前使用的參數#\n",
    "#資料比例:7:1:2\n",
    "#Validation:Independet\n",
    "#RF:n_estimators=500,max_features='log2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3c636-462d-4664-853a-9ef00982a505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
