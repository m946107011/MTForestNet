{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0305f6f4-e7aa-4787-a2fb-1f7c8f064d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deepchem\n",
    "deepchem.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a782809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " /device:CPU:0 || Unnamed device || CPU || 256.0 MiB\n",
      " /device:GPU:0 || Unnamed device || GPU || 44.8 GiB\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\" \n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "devices = device_lib.list_local_devices()\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for d in devices:\n",
    "    t = d.device_type\n",
    "    name = d.physical_device_desc\n",
    "    l = [item.split(':',1) for item in name.split(\", \")]\n",
    "    name_attr = dict([x for x in l if len(x)==3])\n",
    "    dev = name_attr.get('name', 'Unnamed device')\n",
    "    print(f\" {d.name} || {dev} || {t} || {sizeof_fmt(d.memory_limit)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93ff3fa-3ca9-4291-86f8-bcf5d7c081fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "ECFP6\n",
      "train\n",
      "eval\n",
      "執行時間:  84.54695558547974\n",
      "100\n",
      "ECFP6\n",
      "train\n",
      "eval\n",
      "執行時間:  523.7280149459839\n",
      "200\n",
      "ECFP6\n",
      "train\n",
      "eval\n",
      "執行時間:  1041.9083275794983\n",
      "500\n",
      "ECFP6\n",
      "train\n",
      "eval\n",
      "執行時間:  2566.5468702316284\n"
     ]
    }
   ],
   "source": [
    "#MTL#\n",
    "import tempfile\n",
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import psutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "set_seed(seed=8, tensorflow=True)\n",
    "\n",
    "epoch_number=['10','100','200','500']\n",
    "#,'100','200','500']\n",
    "\n",
    "for e_n in epoch_number:\n",
    "    print(e_n)\n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    ###result####\n",
    "    number = list(range(0,500))    \n",
    "    result_df=pd.DataFrame(index=number )\n",
    "    col_list=['Task',\n",
    "            'Train AUC', 'Train ACC','Train Balance ACC','Train F1','Train Precision', 'Train Recall','Train Specificity',\n",
    "            'Valid AUC', 'Valid ACC','Valid Balance ACC','Valid F1','Valid Precision', 'Valid Recall','Valid Specificity',\n",
    "            'Test AUC', 'Test ACC', 'Test Balance ACC','Test F1', 'Test Precision','Test Recall','Test Specificity']\n",
    "\n",
    "    df=pd.read_csv('Zebrafish_final202310_withMorphologyALLdata.csv')\n",
    "    tasks=['LEL_MORT','LEL_YSE','LEL_AXIS','LEL_EYE','LEL_SNOU', 'LEL_JAW',\n",
    "        'LEL_OTIC','LEL_PE','LEL_BRAI','LEL_SOMI','LEL_PFIN','LEL_CFIN',\n",
    "        'LEL_PIG','LEL_CIRC', 'LEL_TRUN','LEL_SWIM','LEL_NC','LEL_TR', '18_END_LEC', 'MOR_LEC','SUBLETH_17_END_LEC', 'TOX_SCO',\n",
    "        'MO24', 'DP24', 'SM24', 'NC24','MORT','YSE', 'AXIS', 'EYE',\n",
    "        'SNOU', 'JAW', 'OTIC', 'PE', 'BRAI', 'SOMI', 'PFIN', 'CFIN',\n",
    "        'PIG','CIRC', 'TRUN', 'SWIM','NC','TR','MOV21', 'AUC21','METAB','MIC']\n",
    "    #task=task.drop(columns='Unnamed: 0')\n",
    "    labels=tasks\n",
    "    for i in df.index:\n",
    "        for q in labels:\n",
    "            try:\n",
    "                if df.loc[i,q]== 'ACT':\n",
    "                    df.loc[i,q]=int(1)\n",
    "                if df.loc[i,q]== 'INACT':           \n",
    "                    df.loc[i,q]=int(0)             \n",
    "            except:\n",
    "                pass   \n",
    "\n",
    "    print('ECFP6')\n",
    "    with dc.utils.UniversalNamedTemporaryFile(mode='w') as tmpfile:\n",
    "      df.to_csv(tmpfile.name)\n",
    "      loader = dc.data.CSVLoader(tasks, feature_field=\"SMILES\",\n",
    "                    featurizer=dc.feat.CircularFingerprint(size=1024, radius=3))\n",
    "      dataset = loader.create_dataset(tmpfile.name)\n",
    "    len(dataset)\n",
    "    splitter = dc.splits.RandomStratifiedSplitter()\n",
    "    # Splitting dataset into train and test datasets\n",
    "    train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(dataset,frac_train=0.7, frac_valid=0.1, frac_test=0.2,seed=8)\n",
    "    n_tasks = len(tasks)\n",
    "    n_features = train_dataset.get_data_shape()[0]\n",
    "    \n",
    "    print('train')\n",
    "    model = dc.models.MultitaskClassifier(n_tasks, \n",
    "        n_features,\n",
    "       momentum= 0.9,\n",
    "       learning_rate=0.0001,\n",
    "       dropouts=[0.25,0.15,0.1],\n",
    "       layer_sizes=[200,100,50],\n",
    "       penalty=0.005,\n",
    "       weight_decay_penalty_type='l2')\n",
    "    \n",
    "    e_n=int(e_n)\n",
    "    # Open a strategy scope.\n",
    "    for d in ['/device:GPU:2','/device:GPU:3','/device:GPU:4','/device:GPU:5']:\n",
    "        model.fit(train_dataset,nb_epoch=e_n)\n",
    "    #######\n",
    "\n",
    "    print('eval')\n",
    "    y_true_train=train_dataset.y\n",
    "    y_tr = model.predict(train_dataset)\n",
    "    y_tr_label = (np.array(y_tr) >= 0.5).astype(int)\n",
    "\n",
    "    \n",
    "    y_true_valid = valid_dataset.y\n",
    "    y_valid = model.predict(valid_dataset)\n",
    "    y_valid_label = (np.array(y_valid) >= 0.5).astype(int)\n",
    "\n",
    "    y_true = test_dataset.y\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred_label = (np.array(y_pred) >= 0.5).astype(int)\n",
    "\n",
    "    AUC_metric = dc.metrics.roc_auc_score\n",
    "    ACC_metric = dc.metrics.accuracy_score\n",
    "    PR_metric = dc.metrics.precision_score\n",
    "    RC_metric = dc.metrics.recall_score\n",
    "    F1_metric = dc.metrics.f1_score\n",
    "    SP_metric = dc.metrics.recall_score\n",
    "\n",
    "\n",
    "    for i in range(n_tasks):\n",
    "        AUC_score = AUC_metric(dc.metrics.to_one_hot(y_true_train[:,i]), y_tr[:,i])\n",
    "        ACC_score = ACC_metric(dc.metrics.to_one_hot(y_true_train[:,i]), y_tr_label[:,i])\n",
    "        PR_score = PR_metric(dc.metrics.to_one_hot(y_true_train[:,i]), y_tr_label[:,i], average=None)[1]\n",
    "        RC_score = RC_metric(y_true_train[:,i], y_tr_label[:,i][:,1],pos_label=1)\n",
    "        F1_score = F1_metric(dc.metrics.to_one_hot(y_true_train[:,i]), y_tr_label[:,i], average=None)[1]\n",
    "        SP_score = SP_metric(y_true_train[:,i], y_tr_label[:,i][:,1],pos_label=0)\n",
    "        \n",
    "        result_df.loc[i,'Task']=tasks[i]\n",
    "        result_df.loc[i,'Train AUC']=AUC_score\n",
    "        result_df.loc[i,'Train ACC']=ACC_score\n",
    "        result_df.loc[i,'Train F1']=F1_score\n",
    "        result_df.loc[i,'Train Precision']=PR_score\n",
    "        result_df.loc[i,'Train Recall']=RC_score    \n",
    "        result_df.loc[i,'Train Specificity']=SP_score    \n",
    "        \n",
    "        AUC_score = AUC_metric(dc.metrics.to_one_hot(y_true_valid[:,i]), y_valid[:,i])\n",
    "        ACC_score = ACC_metric(dc.metrics.to_one_hot(y_true_valid[:,i]), y_valid_label[:,i])\n",
    "        PR_score = PR_metric(dc.metrics.to_one_hot(y_true_valid[:,i]),y_valid_label[:,i], average=None)[1]\n",
    "        RC_score = RC_metric(y_true_valid[:,i], y_valid_label[:,i][:,1],pos_label=1)\n",
    "        F1_score = F1_metric(dc.metrics.to_one_hot(y_true_valid[:,i]), y_valid_label[:,i], average=None)[1]\n",
    "        SP_score = SP_metric(y_true_valid[:,i], y_valid_label[:,i][:,1],pos_label=0)\n",
    "\n",
    "        result_df.loc[i,'Valid AUC']=AUC_score\n",
    "        result_df.loc[i,'Valid ACC']=ACC_score\n",
    "        result_df.loc[i,'Valid F1']=F1_score\n",
    "        result_df.loc[i,'Valid Precision']=PR_score\n",
    "        result_df.loc[i,'Valid Recall']=RC_score    \n",
    "        result_df.loc[i,'Valid Specificity']=SP_score    \n",
    "\n",
    "        AUC_score = AUC_metric(dc.metrics.to_one_hot(y_true[:,i]), y_pred[:,i])\n",
    "        ACC_score = ACC_metric(dc.metrics.to_one_hot(y_true[:,i]), y_pred_label[:,i])\n",
    "        PR_score = PR_metric(dc.metrics.to_one_hot(y_true[:,i]), y_pred_label[:,i], average=None)[1]\n",
    "        F1_score = F1_metric(dc.metrics.to_one_hot(y_true[:,i]), y_pred_label[:,i], average=None)[1]\n",
    "        RC_score = RC_metric(y_true[:,i], y_pred_label[:,i][:,1],pos_label=1)\n",
    "        SP_score = SP_metric(y_true[:,i], y_pred_label[:,i][:,1],pos_label=0)\n",
    "\n",
    "        result_df.loc[i,'Test AUC']=AUC_score\n",
    "        result_df.loc[i,'Test ACC']=ACC_score\n",
    "        result_df.loc[i,'Test F1']=F1_score\n",
    "        result_df.loc[i,'Test Precision']=PR_score\n",
    "        result_df.loc[i,'Test Recall']=RC_score\n",
    "        result_df.loc[i,'Test Specificity']=SP_score    \n",
    "\n",
    "    mean=result_df.mean()\n",
    "    mean=mean.tolist()\n",
    "    mean.insert(0,'AVG')\n",
    "    column_headers = list(result_df.columns.values)\n",
    "    for i in range(len(column_headers)):\n",
    "        result_df.iloc[48,i]=mean[i]    \n",
    "    result_df=result_df.dropna()\n",
    "    end = time.time()\n",
    "    print('Time ',end - start)\n",
    "    \n",
    "    result_df.to_csv('DChem_FinalFishData_MTL_LS215_'+str(e_n)+'.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f3efc4-a6f5-4a82-9dd0-c8d2c9bce8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import psutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "set_seed(seed=8, tensorflow=True, pytorch=True)\n",
    "\n",
    "\n",
    "epoch_number=['10','100','200','500']\n",
    "\n",
    "\n",
    "for e_n in epoch_number:\n",
    "    print(e_n)\n",
    "\n",
    "    start = time.time()\n",
    "    ###result####\n",
    "    number = list(range(0,500))    \n",
    "    result_df=pd.DataFrame(index=number )\n",
    "    col_list=['Task',\n",
    "            'Train AUC', 'Train ACC','Train Balance ACC','Train F1','Train Precision', 'Train Recall','Train Specificity',\n",
    "            'Valid AUC', 'Valid ACC','Valid Balance ACC','Valid F1','Valid Precision', 'Valid Recall','Valid Specificity',\n",
    "            'Test AUC', 'Test ACC', 'Test Balance ACC','Test F1', 'Test Precision','Test Recall','Test Specificity']\n",
    "\n",
    "    df=pd.read_csv('Zebrafish_final202310_withMorphologyALLdata.csv')\n",
    "    tasks=['LEL_MORT','LEL_YSE','LEL_AXIS','LEL_EYE','LEL_SNOU', 'LEL_JAW',\n",
    "        'LEL_OTIC','LEL_PE','LEL_BRAI','LEL_SOMI','LEL_PFIN','LEL_CFIN',\n",
    "        'LEL_PIG','LEL_CIRC', 'LEL_TRUN','LEL_SWIM','LEL_NC','LEL_TR', '18_END_LEC', 'MOR_LEC','SUBLETH_17_END_LEC', 'TOX_SCO',\n",
    "        'MO24', 'DP24', 'SM24', 'NC24','MORT','YSE', 'AXIS', 'EYE',\n",
    "        'SNOU', 'JAW', 'OTIC', 'PE', 'BRAI', 'SOMI', 'PFIN', 'CFIN',\n",
    "        'PIG','CIRC', 'TRUN', 'SWIM','NC','TR','MOV21', 'AUC21','METAB','MIC']\n",
    "    #task=task.drop(columns='Unnamed: 0')\n",
    "    labels=tasks\n",
    "    for i in df.index:\n",
    "        for q in labels:\n",
    "            try:\n",
    "                if df.loc[i,q]== 'ACT':\n",
    "                    df.loc[i,q]=int(1)\n",
    "                if df.loc[i,q]== 'INACT':           \n",
    "                    df.loc[i,q]=int(0)             \n",
    "            except:\n",
    "                pass   \n",
    "\n",
    "    print('ECFP6')\n",
    "    with dc.utils.UniversalNamedTemporaryFile(mode='w') as tmpfile:\n",
    "      df.to_csv(tmpfile.name)\n",
    "      loader = dc.data.CSVLoader(tasks, feature_field=\"SMILES\",\n",
    "                    featurizer=dc.feat.CircularFingerprint(size=1024, radius=3))\n",
    "      dataset = loader.create_dataset(tmpfile.name)\n",
    "\n",
    "    len(dataset)\n",
    "\n",
    "    splitter = dc.splits.RandomStratifiedSplitter()\n",
    "    # Splitting dataset into train and test datasets\n",
    "    train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(dataset,frac_train=0.7, frac_valid=0.1, frac_test=0.2,seed=8)\n",
    "\n",
    "    n_tasks = len(tasks)\n",
    "    n_features = train_dataset.get_data_shape()[0]\n",
    "    \n",
    "    print('train')\n",
    "    model = dc.models.RobustMultitaskClassifier(n_tasks, \n",
    "        n_features,\n",
    "       momentum= 0.9,\n",
    "       learning_rate=0.0001,\n",
    "       dropouts=[0.25,0.15,0.1],\n",
    "       #[400,100,50]                                         \n",
    "       layer_sizes=[400,200,100],\n",
    "       penalty=0.005,\n",
    "       weight_decay_penalty_type='l2')\n",
    "    \n",
    "    e_n=int(e_n)\n",
    "    # Open a strategy scope.\n",
    "    for d in ['/device:GPU:0']:\n",
    "        model.fit(train_dataset,nb_epoch=e_n)\n",
    "    #######\n",
    "\n",
    "    print('eval')\n",
    "    y_true_train=train_dataset.y\n",
    "    y_tr = model.predict(train_dataset)\n",
    "    y_tr_label = (y_tr >= 0.5) \n",
    "\n",
    "    y_true_valid = valid_dataset.y\n",
    "    y_valid = model.predict(valid_dataset)\n",
    "    y_valid_label = (y_valid >= 0.5) \n",
    "\n",
    "    y_true = test_dataset.y\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred_label = (y_pred >= 0.5) \n",
    "\n",
    "    AUC_metric = dc.metrics.roc_auc_score\n",
    "    ACC_metric = dc.metrics.accuracy_score\n",
    "    PR_metric = dc.metrics.precision_score\n",
    "    RC_metric = dc.metrics.recall_score\n",
    "    F1_metric = dc.metrics.f1_score\n",
    "    SP_metric = dc.metrics.recall_score\n",
    "\n",
    "\n",
    "    for i in range(n_tasks):\n",
    "        AUC_score = AUC_metric(dc.metrics.to_one_hot(y_true_train[:,i]), y_tr[:,i])\n",
    "        ACC_score = ACC_metric(dc.metrics.to_one_hot(y_true_train[:,i]), y_tr_label[:,i])\n",
    "        PR_score = PR_metric(dc.metrics.to_one_hot(y_true_train[:,i]), y_tr_label[:,i], average=None)[1]\n",
    "        RC_score = RC_metric(y_true_train[:,i], y_tr_label[:,i][:,1],pos_label=1)\n",
    "        F1_score = F1_metric(dc.metrics.to_one_hot(y_true_train[:,i]), y_tr_label[:,i], average=None)[1]\n",
    "        SP_score = SP_metric(y_true_train[:,i], y_tr_label[:,i][:,1],pos_label=0)\n",
    "        \n",
    "        result_df.loc[i,'Task']=tasks[i]\n",
    "        result_df.loc[i,'Train AUC']=AUC_score\n",
    "        result_df.loc[i,'Train ACC']=ACC_score\n",
    "        result_df.loc[i,'Train F1']=F1_score\n",
    "        result_df.loc[i,'Train Precision']=PR_score\n",
    "        result_df.loc[i,'Train Recall']=RC_score    \n",
    "        result_df.loc[i,'Train Specificity']=SP_score    \n",
    "\n",
    "        \n",
    "        \n",
    "        AUC_score = AUC_metric(dc.metrics.to_one_hot(y_true_valid[:,i]), y_valid[:,i])\n",
    "        ACC_score = ACC_metric(dc.metrics.to_one_hot(y_true_valid[:,i]), y_valid_label[:,i])\n",
    "        PR_score = PR_metric(dc.metrics.to_one_hot(y_true_valid[:,i]),y_valid_label[:,i], average=None)[1]\n",
    "        RC_score = RC_metric(y_true_valid[:,i], y_valid_label[:,i][:,1],pos_label=1)\n",
    "        F1_score = F1_metric(dc.metrics.to_one_hot(y_true_valid[:,i]), y_valid_label[:,i], average=None)[1]\n",
    "        SP_score = SP_metric(y_true_valid[:,i], y_valid_label[:,i][:,1],pos_label=0)\n",
    "\n",
    "        \n",
    "        result_df.loc[i,'Valid AUC']=AUC_score\n",
    "        result_df.loc[i,'Valid ACC']=ACC_score\n",
    "        result_df.loc[i,'Valid F1']=F1_score\n",
    "        result_df.loc[i,'Valid Precision']=PR_score\n",
    "        result_df.loc[i,'Valid Recall']=RC_score    \n",
    "        result_df.loc[i,'Valid Specificity']=SP_score    \n",
    "\n",
    "\n",
    "        AUC_score = AUC_metric(dc.metrics.to_one_hot(y_true[:,i]), y_pred[:,i])\n",
    "        ACC_score = ACC_metric(dc.metrics.to_one_hot(y_true[:,i]), y_pred_label[:,i])\n",
    "        PR_score = PR_metric(dc.metrics.to_one_hot(y_true[:,i]), y_pred_label[:,i], average=None)[1]\n",
    "        F1_score = F1_metric(dc.metrics.to_one_hot(y_true[:,i]), y_pred_label[:,i], average=None)[1]\n",
    "        RC_score = RC_metric(y_true[:,i], y_pred_label[:,i][:,1],pos_label=1)\n",
    "        SP_score = SP_metric(y_true[:,i], y_pred_label[:,i][:,1],pos_label=0)\n",
    "\n",
    "        \n",
    "        result_df.loc[i,'Test AUC']=AUC_score\n",
    "        result_df.loc[i,'Test ACC']=ACC_score\n",
    "        result_df.loc[i,'Test F1']=F1_score\n",
    "        result_df.loc[i,'Test Precision']=PR_score\n",
    "        result_df.loc[i,'Test Recall']=RC_score\n",
    "        result_df.loc[i,'Test Specificity']=SP_score    \n",
    "\n",
    "    mean=result_df.mean()\n",
    "    mean=mean.tolist()\n",
    "    mean.insert(0,'AVG')\n",
    "    column_headers = list(result_df.columns.values)\n",
    "    for i in range(len(column_headers)):\n",
    "        result_df.iloc[48,i]=mean[i]    \n",
    "    result_df=result_df.dropna()\n",
    "    end = time.time()\n",
    "    print('Time: ',end - start)\n",
    "    \n",
    "    result_df.to_csv('DChem_FinalFishData_RB_MTL_LS421_'+str(e_n)+'.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc142862-accd-447f-a22a-8ea6a5decbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import psutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "set_seed(seed=8, tensorflow=True)\n",
    "import tensorflow as tf\n",
    "\n",
    "# List all available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        # Allow GPU memory growth\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Now, continue with your TensorFlow code\n",
    "\n",
    "\n",
    "\n",
    "epoch_number=['500','10','100','200']\n",
    "\n",
    "for e_n in epoch_number:\n",
    "    print(e_n)\n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    \n",
    "    ###result####\n",
    "    number = list(range(0,500))    \n",
    "    result_df=pd.DataFrame(index=number )\n",
    "    col_list=['Task',\n",
    "            'Train AUC', 'Train ACC','Train Balance ACC','Train F1','Train Precision', 'Train Recall','Train Specificity',\n",
    "            'Valid AUC', 'Valid ACC','Valid Balance ACC','Valid F1','Valid Precision', 'Valid Recall','Valid Specificity',\n",
    "            'Test AUC', 'Test ACC', 'Test Balance ACC','Test F1', 'Test Precision','Test Recall','Test Specificity']\n",
    "\n",
    "    df=pd.read_csv('Zebrafish_final202310_withMorphologyALLdata.csv')\n",
    "    tasks=['LEL_MORT','LEL_YSE','LEL_AXIS','LEL_EYE','LEL_SNOU', 'LEL_JAW',\n",
    "        'LEL_OTIC','LEL_PE','LEL_BRAI','LEL_SOMI','LEL_PFIN','LEL_CFIN',\n",
    "        'LEL_PIG','LEL_CIRC', 'LEL_TRUN','LEL_SWIM','LEL_NC','LEL_TR', '18_END_LEC', 'MOR_LEC','SUBLETH_17_END_LEC', 'TOX_SCO',\n",
    "        'MO24', 'DP24', 'SM24', 'NC24','MORT','YSE', 'AXIS', 'EYE',\n",
    "        'SNOU', 'JAW', 'OTIC', 'PE', 'BRAI', 'SOMI', 'PFIN', 'CFIN',\n",
    "        'PIG','CIRC', 'TRUN', 'SWIM','NC','TR','MOV21', 'AUC21','METAB','MIC']\n",
    "    #task=task.drop(columns='Unnamed: 0')\n",
    "    labels=tasks\n",
    "    for i in df.index:\n",
    "        for q in labels:\n",
    "            try:\n",
    "                if df.loc[i,q]== 'ACT':\n",
    "                    df.loc[i,q]=int(1)\n",
    "                if df.loc[i,q]== 'INACT':           \n",
    "                    df.loc[i,q]=int(0)             \n",
    "            except:\n",
    "                pass   \n",
    "\n",
    "    print('ECFP6')\n",
    "    with dc.utils.UniversalNamedTemporaryFile(mode='w') as tmpfile:\n",
    "      df.to_csv(tmpfile.name)\n",
    "      loader = dc.data.CSVLoader(tasks, feature_field=\"SMILES\",\n",
    "                    featurizer=dc.feat.CircularFingerprint(size=1024, radius=3))\n",
    "      dataset = loader.create_dataset(tmpfile.name)\n",
    "\n",
    "    len(dataset)\n",
    "\n",
    "    splitter = dc.splits.RandomStratifiedSplitter()\n",
    "    # Splitting dataset into train and test datasets\n",
    "    train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(dataset,frac_train=0.7, frac_valid=0.1, frac_test=0.2,seed=8)\n",
    "\n",
    "    #train_dataset, test_dataset = splitter.train_test_split(dataset,frac_train = 0.8,seed=8)\n",
    "\n",
    "\n",
    "    n_tasks = len(tasks)\n",
    "    n_features = train_dataset.get_data_shape()[0]\n",
    "    \n",
    "    print('train')\n",
    "     # Create a MirroredStrategy.\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    model = dc.models.ProgressiveMultitaskClassifier(n_tasks, n_features,\n",
    "       mode=\"classification\",       \n",
    "        n_weave=2, \n",
    "       momentum= 0.9,\n",
    "       learning_rate=0.0001,\n",
    "       dropouts=[0.25,0.15,0.1],\n",
    "       layer_sizes=[200,100,50],\n",
    "            #[400,100,50]'\n",
    "       penalty=0.005,\n",
    "       weight_decay_penalty_type='l2',\n",
    "       batch_normalize=False)\n",
    "\n",
    "    e_n=int(e_n)\n",
    "    # Open a strategy scope.\n",
    "    for d in ['/device:GPU:0','/device:GPU:1','/device:GPU:2']:\n",
    "        model.fit(train_dataset, nb_epoch=e_n)\n",
    "    #######\n",
    "\n",
    "    print('eval')\n",
    "    y_true_train=train_dataset.y\n",
    "    y_tr = model.predict(train_dataset)\n",
    "    y_tr_label = (y_tr >= 0.5) \n",
    "\n",
    "    y_true_valid = valid_dataset.y\n",
    "    y_valid = model.predict(valid_dataset)\n",
    "    y_valid_label = (y_valid >= 0.5) \n",
    "\n",
    "    y_true = test_dataset.y\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred_label = (y_pred >= 0.5) \n",
    "\n",
    "    AUC_metric = dc.metrics.roc_auc_score\n",
    "    ACC_metric = dc.metrics.accuracy_score\n",
    "    PR_metric = dc.metrics.precision_score\n",
    "    RC_metric = dc.metrics.recall_score\n",
    "    F1_metric = dc.metrics.f1_score\n",
    "    SP_metric = dc.metrics.recall_score\n",
    "\n",
    "\n",
    "    for i in range(n_tasks):\n",
    "        AUC_score = AUC_metric(dc.metrics.to_one_hot(y_true_train[:,i]), y_tr[:,i])\n",
    "        ACC_score = ACC_metric(dc.metrics.to_one_hot(y_true_train[:,i]), y_tr_label[:,i])\n",
    "        PR_score = PR_metric(dc.metrics.to_one_hot(y_true_train[:,i]), y_tr_label[:,i], average=None)[1]\n",
    "        RC_score = RC_metric(y_true_train[:,i], y_tr_label[:,i][:,1],pos_label=1)\n",
    "        F1_score = F1_metric(dc.metrics.to_one_hot(y_true_train[:,i]), y_tr_label[:,i], average=None)[1]\n",
    "        SP_score = SP_metric(y_true_train[:,i], y_tr_label[:,i][:,1],pos_label=0)\n",
    "        \n",
    "        result_df.loc[i,'Task']=tasks[i]\n",
    "        result_df.loc[i,'Train AUC']=AUC_score\n",
    "        result_df.loc[i,'Train ACC']=ACC_score\n",
    "        result_df.loc[i,'Train F1']=F1_score\n",
    "        result_df.loc[i,'Train Precision']=PR_score\n",
    "        result_df.loc[i,'Train Recall']=RC_score    \n",
    "        result_df.loc[i,'Train Specificity']=SP_score    \n",
    "\n",
    "        \n",
    "        \n",
    "        AUC_score = AUC_metric(dc.metrics.to_one_hot(y_true_valid[:,i]), y_valid[:,i])\n",
    "        ACC_score = ACC_metric(dc.metrics.to_one_hot(y_true_valid[:,i]), y_valid_label[:,i])\n",
    "        PR_score = PR_metric(dc.metrics.to_one_hot(y_true_valid[:,i]),y_valid_label[:,i], average=None)[1]\n",
    "        RC_score = RC_metric(y_true_valid[:,i], y_valid_label[:,i][:,1],pos_label=1)\n",
    "        F1_score = F1_metric(dc.metrics.to_one_hot(y_true_valid[:,i]), y_valid_label[:,i], average=None)[1]\n",
    "        SP_score = SP_metric(y_true_valid[:,i], y_valid_label[:,i][:,1],pos_label=0)\n",
    "\n",
    "        \n",
    "        result_df.loc[i,'Valid AUC']=AUC_score\n",
    "        result_df.loc[i,'Valid ACC']=ACC_score\n",
    "        result_df.loc[i,'Valid F1']=F1_score\n",
    "        result_df.loc[i,'Valid Precision']=PR_score\n",
    "        result_df.loc[i,'Valid Recall']=RC_score    \n",
    "        result_df.loc[i,'Valid Specificity']=SP_score    \n",
    "\n",
    "\n",
    "        AUC_score = AUC_metric(dc.metrics.to_one_hot(y_true[:,i]), y_pred[:,i])\n",
    "        ACC_score = ACC_metric(dc.metrics.to_one_hot(y_true[:,i]), y_pred_label[:,i])\n",
    "        PR_score = PR_metric(dc.metrics.to_one_hot(y_true[:,i]), y_pred_label[:,i], average=None)[1]\n",
    "        F1_score = F1_metric(dc.metrics.to_one_hot(y_true[:,i]), y_pred_label[:,i], average=None)[1]\n",
    "        RC_score = RC_metric(y_true[:,i], y_pred_label[:,i][:,1],pos_label=1)\n",
    "        SP_score = SP_metric(y_true[:,i], y_pred_label[:,i][:,1],pos_label=0)\n",
    "\n",
    "        \n",
    "        result_df.loc[i,'Test AUC']=AUC_score\n",
    "        result_df.loc[i,'Test ACC']=ACC_score\n",
    "        result_df.loc[i,'Test F1']=F1_score\n",
    "        result_df.loc[i,'Test Precision']=PR_score\n",
    "        result_df.loc[i,'Test Recall']=RC_score\n",
    "        result_df.loc[i,'Test Specificity']=SP_score    \n",
    "\n",
    "    mean=result_df.mean()\n",
    "    mean=mean.tolist()\n",
    "    mean.insert(0,'AVG')\n",
    "    column_headers = list(result_df.columns.values)\n",
    "    for i in range(len(column_headers)):\n",
    "        result_df.iloc[48,i]=mean[i]    \n",
    "    result_df=result_df.dropna()\n",
    "    end = time.time()\n",
    "    print('TIME: ',end - start)\n",
    "\n",
    "    \n",
    "    result_df.to_csv('DChem_FinalFishData_PRMTL_LS215_'+str(e_n)+'.csv')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
